"use strict";(self.webpackChunkstryker_mutator_github_io=self.webpackChunkstryker_mutator_github_io||[]).push([[618],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return h}});var s=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);t&&(s=s.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,s)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,s,r=function(e,t){if(null==e)return{};var n,s,r={},a=Object.keys(e);for(s=0;s<a.length;s++)n=a[s],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(s=0;s<a.length;s++)n=a[s],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=s.createContext({}),c=function(e){var t=s.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=c(e.components);return s.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return s.createElement(s.Fragment,{},t)}},p=s.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,l=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),p=c(n),h=r,m=p["".concat(l,".").concat(h)]||p[h]||d[h]||a;return n?s.createElement(m,i(i({ref:t},u),{},{components:n})):s.createElement(m,i({ref:t},u))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,i=new Array(a);i[0]=p;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:r,i[1]=o;for(var c=2;c<a;c++)i[c]=n[c];return s.createElement.apply(null,i)}return s.createElement.apply(null,n)}p.displayName="MDXCreateElement"},3976:function(e,t,n){n.r(t),n.d(t,{assets:function(){return l},contentTitle:function(){return i},default:function(){return d},frontMatter:function(){return a},metadata:function(){return o},toc:function(){return c}});var s=n(7462),r=(n(7294),n(3905));const a={title:"Testing frameworks",sidebar_position:50,custom_edit_url:"https://github.com/stryker-mutator/stryker-net/edit/master/docs/technical-reference/testing-frameworks.md"},i="Testing Frameworks undocumented",o={unversionedId:"stryker-net/technical-reference/testing-framework",id:"stryker-net/technical-reference/testing-framework",title:"Testing frameworks",description:"This document captures the discoveries made about the various testing frameworks while working on Stryker.",source:"@site/docs/stryker-net/technical-reference/testing-framework.md",sourceDirName:"stryker-net/technical-reference",slug:"/stryker-net/technical-reference/testing-framework",permalink:"/docs/stryker-net/technical-reference/testing-framework",draft:!1,editUrl:"https://github.com/stryker-mutator/stryker-net/edit/master/docs/technical-reference/testing-frameworks.md",tags:[],version:"current",sidebarPosition:50,frontMatter:{title:"Testing frameworks",sidebar_position:50,custom_edit_url:"https://github.com/stryker-mutator/stryker-net/edit/master/docs/technical-reference/testing-frameworks.md"},sidebar:"docs",previous:{title:"Project components",permalink:"/docs/stryker-net/technical-reference/project-components"},next:{title:"Classes",permalink:"/docs/stryker-net/technical-reference/fsharp/classes"}},l={},c=[{value:"Generalities",id:"generalities",level:2},{value:"Identification",id:"identification",level:2},{value:"xUnit",id:"xunit",level:2},{value:"Theories",id:"theories",level:3},{value:"Static theories",id:"static-theories",level:4},{value:"Run time theories",id:"run-time-theories",level:4},{value:"NUnit",id:"nunit",level:3},{value:"TestCases",id:"testcases",level:3},{value:"TestCase",id:"testcase",level:3},{value:"TestCaseSource",id:"testcasesource",level:3}],u={toc:c};function d(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,s.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"testing-frameworks-undocumented"},"Testing Frameworks undocumented"),(0,r.kt)("p",null,"This document captures the discoveries made about the various testing frameworks while working on Stryker.\nThis can help understanding the design and logic of the mutation test classes.\nThis is a mandatory reading for anyone trying to understand and/or fix bugs related to coverage capture, analysis and testing phase."),(0,r.kt)("p",null,"To be specific, this document will mostly (and implicitly) refer to their VsTest adapters."),(0,r.kt)("h2",{id:"generalities"},"Generalities"),(0,r.kt)("p",null,"The ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Microsoft/vstest-docs/blob/master/RFCs/0006-DataCollection-Protocol.md"},"VsTest protocol")," is not\nrich enough to cover every test framework feature. Therefore, each framework makes their own decisions on how to handle\nsome of their specificities. That is the reason why we can observe different behaviour and surprising results."),(0,r.kt)("p",null,"This affects:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"how tests are discovered"),(0,r.kt)("li",{parentName:"ul"},"how tests are described/identified"),(0,r.kt)("li",{parentName:"ul"},"how tests are filtered"),(0,r.kt)("li",{parentName:"ul"},"how tests are executed"),(0,r.kt)("li",{parentName:"ul"},"how test results are reported")),(0,r.kt)("h2",{id:"identification"},"Identification"),(0,r.kt)("p",null,"By default, VsTest generates unique identifiers stored as ",(0,r.kt)("inlineCode",{parentName:"p"},"Guid"),". I do not know if test runners are able to provide\na specific implementation but in practice, this identifier is derived from the(test's) display's name hash code."),(0,r.kt)("h2",{id:"xunit"},"xUnit"),(0,r.kt)("h3",{id:"theories"},"Theories"),(0,r.kt)("p",null,"xUnit's theory is a pattern where some test is executed multiple times with varying input data (like NUnit's TestCase).\nThese test mesthods accept one or more parameters and bear one of this attributes: ",(0,r.kt)("inlineCode",{parentName:"p"},"InlineData"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"MemberData")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"ClassData"),".\nThere are differences\nbetween compile time theories (data is fixed at build time) and run time theories (data is discovered at run time)."),(0,r.kt)("h4",{id:"static-theories"},"Static theories"),(0,r.kt)("p",null,"Static discoveries are seen and processed as different test cases."),(0,r.kt)("h4",{id:"run-time-theories"},"Run time theories"),(0,r.kt)("p",null,"Run time theories may be discovered as one test, disregarding the number of underlying data set.\nDuring discovery phase, xUnit determines if it can generates a test case for each (theory) data set;\nif it can, those test cases are processed as usual; otherwise it will provide several results for the same test case."),(0,r.kt)("p",null,"There is one test result per dataset, all associated with the same test case."),(0,r.kt)("p",null,"Here is a summarized timeline of tests execution:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"   xUnit runner calls data theory's data source to fetch each test case value(s)\nTestCase start event\n   xUnit run test with first set of data\n   xUnit run test with second set of data\n   ...\n   xUnit run test with last set of data\n   xUnit reports first test results\nTestCase end event\n   xUnit reports second test reult\n   ...\n   xUnit reports last test result\nTestCase start event\n   ...\n")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Remark"),": in this situation, each test result includes the TOTAL execution time (i.e. the time to test every data set) so one should not sum them."),(0,r.kt)("p",null,"The difficulty here is that a lot happens between ",(0,r.kt)("inlineCode",{parentName:"p"},"testcase end")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"testcase start")," events.\nAt coverage capture it is a problem, because there is a risk of spilling coverage information to the next tests: Stryker\ncan only capture coverage information on ",(0,r.kt)("inlineCode",{parentName:"p"},"testcase end")," events, and only in association with the current running test.\nSo, every mutants covered between ",(0,r.kt)("inlineCode",{parentName:"p"},"testcase end")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"testcase start")," will be associated with the ",(0,r.kt)("strong",{parentName:"p"},"next")," test."),(0,r.kt)("p",null,"And during execution phase, it is impossible to predict when the test will really be over, so it is difficult to\nestablish if the test was succesful.  "),(0,r.kt)("p",null,"Also, if a mutation ends up changing a test case name - typically by changing the result of ",(0,r.kt)("inlineCode",{parentName:"p"},"ToTring()"),", it will change the\ntest identifier so this testcase will only run when running ",(0,r.kt)("strong",{parentName:"p"},"all tests")," and can no longer be executed in isolation, as\nStryker can't anticipate the test name."),(0,r.kt)("h3",{id:"nunit"},"NUnit"),(0,r.kt)("h3",{id:"testcases"},"TestCases"),(0,r.kt)("p",null,"NUnit's TestCases is a pattern where some test is executed multiple times with varying input data (like xUnit's TestCase).\nThese test methods accept one or more parameters and bear one of these attributes (on top of the ",(0,r.kt)("inlineCode",{parentName:"p"},"[Test]")," attribute):\n",(0,r.kt)("inlineCode",{parentName:"p"},"TestCase")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"TestCaseSource"),"."),(0,r.kt)("p",null,"There are some differences\nbetween ",(0,r.kt)("inlineCode",{parentName:"p"},"TestCase")," (data is fixed at build time) and ",(0,r.kt)("inlineCode",{parentName:"p"},"TestCaseSource")," (data is discovered at run time)."),(0,r.kt)("h3",{id:"testcase"},"TestCase"),(0,r.kt)("p",null,"Basically, each ",(0,r.kt)("inlineCode",{parentName:"p"},"TestCase")," is considered to be a specific test, and NUnit generates a name for each of those using the\n",(0,r.kt)("inlineCode",{parentName:"p"},"ToString()")," method for each parameter (e.g: ",(0,r.kt)("inlineCode",{parentName:"p"},"Test(1,2)")," for parameter 1 and 2).\nThen each test is run separately and a test result is provided for each test.\n",(0,r.kt)("strong",{parentName:"p"},"Caveat"),": if the ",(0,r.kt)("inlineCode",{parentName:"p"},"ToString()")," method is not overriden, multiple test cases will have the same name\n(eg: ",(0,r.kt)("inlineCode",{parentName:"p"},"Test(MyClass"),"). Then, there will be no way to distinguish between tests as well as properly associate test\nresults."),(0,r.kt)("h3",{id:"testcasesource"},"TestCaseSource"),(0,r.kt)("p",null,"Each test case is reported during the discovery phase, and NUnit generates a name for each of those using the\n",(0,r.kt)("inlineCode",{parentName:"p"},"ToString()")," method for each parameter (e.g: ",(0,r.kt)("inlineCode",{parentName:"p"},"Test(1,2)")," for parameter 1 and 2).\nThen each test is run separately and a test result is provided for each test.\n",(0,r.kt)("strong",{parentName:"p"},"Caveat"),": if the ",(0,r.kt)("inlineCode",{parentName:"p"},"ToString()")," method is not overriden, multiple test cases will have the same name\n(eg: ",(0,r.kt)("inlineCode",{parentName:"p"},"Test(MyClass"),"). Then, there will be no way to distinguish between tests as well as properly associate test\nresults.\nWhen this happens, these appear as a single test resulting in multiple outcomes in Visual Studio."),(0,r.kt)("p",null,"Here is a summarized timeline of tests execution:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"   NUnit runner calls the TestCaseSource and gets every test case.\nTestCase start event\n   NUnit run test with first set of data\n   NUnit reports test reult\nTestCase end event\nTestCase start event\n   NUnit run test with second set of data\n   NUnit reports test reult\nTestCase end event\n   ...\n   NUnit reports lasy test reult\nTestCase end event\nTestCase start event\n   ...\n")),(0,r.kt)("p",null,"The problems for Stryker are:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"If test cases have the same name, we cannot distinguish between test cases, so it must be handled as a single test. This ",(0,r.kt)("em",{parentName:"li"},"single test")," reports multiple test cases so Stryker needs to wait either for one failure or for each testcase\nto succeed to establish the correct status. The good news is that discovery should provide the number of results to wait for."),(0,r.kt)("li",{parentName:"ol"},"Every mutation encountered by the ",(0,r.kt)("inlineCode",{parentName:"li"},"TestCaseSource")," method will be reported as covered by the first test case,\nwhereas it is likely these mutations would need to be adequately associated with their realtive test case."),(0,r.kt)("li",{parentName:"ol"},"TestCaseSource is called ",(0,r.kt)("strong",{parentName:"li"},"before")," Testcase start event which means Stryker may set the active mutation too late.\nThe only way to handle this is to force any concerned mutants to run in dedicated test sessions.")))}d.isMDXComponent=!0}}]);